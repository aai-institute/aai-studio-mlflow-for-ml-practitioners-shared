{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5010f10e-6555-4abc-a11a-5602bbda7400",
   "metadata": {},
   "source": [
    "# Bike Sharing Demand Prediction\n",
    "\n",
    "The following notebook is a typical example for a data science workflow that does not \n",
    "yet use experiment tracking. It will serve as a starting point for our exercises. Your \n",
    "task will be to add experiment tracking to the notebook. \n",
    "\n",
    "Let's first go over the steps of the workflow:\n",
    "\n",
    "- Loading Data \n",
    "- Splitting the dataset \n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Define numerical and categorical features\n",
    "- Train-test split\n",
    "- Build a reusable training pipeline\n",
    "- Train a model using linear regression\n",
    "- Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a488b30f-be60-4209-9343-a8b18fa1d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11abca2a-0d6c-41ad-9ee5-1f2002f37e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578238b2-e34c-43ea-bac4-c39e9727743f",
   "metadata": {},
   "source": [
    "## Step 1: Loading the Data Set into a Pandas DataFrame\n",
    "\n",
    "- Load the dataset into a Pandas DataFrame\n",
    "- Inspect (part of) the dataset using the .head() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786741ec-6cec-45ec-b801-c4105507b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_sharing_data = pd.read_csv(\"../data/hour.csv\")\n",
    "bike_sharing_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc32e4e8-1ef3-4d7a-95ac-d5ed53c707c7",
   "metadata": {},
   "source": [
    "## Step 2: Split the Dataset by Year\n",
    "\n",
    "- Split the dataset by year. The columns 'yr' is used to indicate the year using\n",
    "- 0 = 2011 and 1 = 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a35db0-c3cc-4aa3-affb-8c9722062f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2011 = bike_sharing_data[bike_sharing_data[\"yr\"] == 0]\n",
    "data_2012 = bike_sharing_data[bike_sharing_data[\"yr\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597bbd47-d471-40a8-b03c-67c11791189f",
   "metadata": {},
   "source": [
    "## Step 3: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9037df7-c6f8-4d5a-95c0-cf7e12188fae",
   "metadata": {},
   "source": [
    "### Step 3.1: Plot the distribution of the target variable\n",
    "\n",
    "- Plot the distribution of the target variable ('cnt') using sns.displot().\n",
    "- Plot absolute counts or the probability density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216aa523-cc91-48b1-8b84-4cea2add22a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data_2011[\"cnt\"], kde=True, stat=\"density\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Probability density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae560708-1347-4cdc-a165-e5286471b7d3",
   "metadata": {},
   "source": [
    "### Step 3.2: Investigate feature-target relationships\n",
    "\n",
    "- Feel free to try out different features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24590b9-cd66-4a4b-a92b-2224366459f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=data_2011, x=\"season\", y=\"cnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3574e3-02fa-4fbd-ba7a-11ef53dd3cf8",
   "metadata": {},
   "source": [
    "## Step 4: Define Numerical and Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba1ded30-2da4-454a-9ab8-57cfc473f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL_FEATURES = [\"temp\", \"hum\", \"windspeed\"]\n",
    "CATEGORICAL_FEATURES = [\"season\", \"mnth\", \"hr\", \"holiday\", \"weekday\", \"workingday\", \"weathersit\"]\n",
    "\n",
    "FEATURES = NUMERICAL_FEATURES + CATEGORICAL_FEATURES\n",
    "\n",
    "TARGET = \"cnt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763a5b7-12eb-4908-a742-d102f132e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_target = data_2011[FEATURES + [TARGET]]\n",
    "features_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd54f8d-0737-40a5-9781-e466a2539e4f",
   "metadata": {},
   "source": [
    "## Step 5: Train-test Split\n",
    "\n",
    "\n",
    "- Split the dataset into a training and testing dataset\n",
    "- Split both the train and test sets into \"input\" and \"output\" (i.e, \"train_input\", \"train_output\", etc.)\n",
    "    - The \"input\" will contain all the features and will serve as the input to the model training and the predictions.\n",
    "    - The \"output\" will be used as target in the model training and as ground truth in the evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd146f27-0fd6-4286-b2c3-0b8aa10bb37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(features_target, random_state=RANDOM_STATE)\n",
    "\n",
    "X_train = train_data[FEATURES]\n",
    "y_train = train_data[TARGET]\n",
    "\n",
    "X_test = test_data[FEATURES]\n",
    "y_test = test_data[TARGET]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a619c4-ceb5-485a-84db-a91b3051949d",
   "metadata": {},
   "source": [
    "## Step 6: Building a Reusable Training Pipeline\n",
    "\n",
    "Define a function that builds a reusable training pipeline. The functions\n",
    "takes an estimator and returns a Scikit-Learn pipeline containing the preprocessing\n",
    "and model training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35e9f2db-34fe-453b-af24-302cc7f3d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = Pipeline([\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"encoder\", OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"numerical\", numerical_transformer, NUMERICAL_FEATURES),\n",
    "    (\"categorical\", categorical_transformer, CATEGORICAL_FEATURES),\n",
    "])\n",
    "\n",
    "def build_pipeline(estimator):\n",
    "    return Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"estimator\", estimator),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea2ece-251f-4a33-834e-b66c3d1f788b",
   "metadata": {},
   "source": [
    "## Step 7: Train a Model using Linear Regression\n",
    "\n",
    "Create a linear regression pipeline and fit the pipeline to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d94ce-01ca-4d55-a738-5064c9808845",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = build_pipeline(LinearRegression())\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfeae62-4371-4354-a562-6b1b089d9f2b",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db1033a0-6baf-4761-af20-edb90cf77235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    log_diff = np.log1p(y_pred) - np.log1p(y_true)\n",
    "    squared_log_diff = log_diff ** 2\n",
    "    mean_squared_log_diff = np.mean(squared_log_diff)\n",
    "    return np.sqrt(mean_squared_log_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80e042db-d4e1-497f-8cf7-0a88416d5fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmsle = root_mean_squared_log_error(y_true, y_pred)\n",
    "\n",
    "    print(f\"RMSE: {rmse:.5f}\")\n",
    "    print(f\"RÂ² score: {r2:.5f}\")\n",
    "    print(f\"MAE: {mae:.5f}\")\n",
    "    print(f\"RMSLE: {rmsle:0.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f2c992-b36c-4fe6-9af9-ee0da74d0b33",
   "metadata": {},
   "source": [
    "## Step 9: Handle Skewed Training Data with Log Transform\n",
    "\n",
    "- Recreate the linear regression pipeline and fit it to the log transformed data\n",
    "- Create predictions using the new model\n",
    "- Inverse the log transformation on the predictions\n",
    "- Evaluate the model performance using the evaluate() function and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c5d8161-892b-43af-bcc4-9a4ad566033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log_transformed = np.log1p(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c5a13-33be-4e7d-9aff-401026a91bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = build_pipeline(LinearRegression())\n",
    "pipeline.fit(X_train, y_train_log_transformed)\n",
    "\n",
    "predictions = pipeline.predict(X_test)\n",
    "predictions = np.expm1(predictions)\n",
    "\n",
    "calculate_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8525d5f-8feb-452e-b6a9-a52dd76b3f09",
   "metadata": {},
   "source": [
    "## Step 10: Different Model - Random Forest Regressor\n",
    "\n",
    "- Create a random forest regressor pipeline.\n",
    "- Fit the pipeline, create predictions, and evaluate the model.\n",
    "- Compare the model performance between the two different model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e67bf-4c5d-4a09-a915-ee297015d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = build_pipeline(RandomForestRegressor(random_state=RANDOM_STATE))\n",
    "pipeline.fit(X_train, y_train_log_transformed)\n",
    "\n",
    "predictions = pipeline.predict(X_test)\n",
    "predictions = np.expm1(predictions)\n",
    "\n",
    "calculate_metrics(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
